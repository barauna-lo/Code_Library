{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barauna-lo/Code_Library/blob/main/visualizing_mnist_using_a_variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "5bff66d63dca5b4b08f9c69e554a6177895dc670",
        "_cell_guid": "b14df16e-0a02-49a8-a477-7cd84f80fc0e",
        "id": "xmCwyRRaJ72A"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualizing MNIST with a Deep Variational Autoencoder\n",
        "\n",
        "1. Introduction  \n",
        "    A. What is autoencoding?  \n",
        "    B. Autoencoders  \n",
        "    C. The Variational Variety  \n",
        "2. Data preparation  \n",
        "    A. Load data  \n",
        "    B. Combine train & test  \n",
        "    C. Split into new training & validation sets  \n",
        "    D. Reshape & normalize  \n",
        "3. Model construction  \n",
        "    A. Encoder network  \n",
        "    B. Sampling function  \n",
        "    C. Decoder network  \n",
        "    D. Loss\n",
        "4. Train the VAE\n",
        "5. Results  \n",
        "    A. Clustering of digits in the latent space  \n",
        "    B. Sample digits\n",
        "\n",
        "    https://www.kaggle.com/code/rvislaywade/visualizing-mnist-using-a-variational-autoencoder"
      ]
    },
    {
      "metadata": {
        "_uuid": "9184c1178ebfec6552bcc5ced94b98a22df9a24b",
        "_cell_guid": "9232ab89-1026-4c9e-93d1-dd9a7e3f8f8f",
        "id": "TbEziLbEJ72E"
      },
      "cell_type": "markdown",
      "source": [
        "## *1. Introduction*\n",
        "I spent a rainy weekend recently learning about autoencoders using TensorFlow and Keras in Python. I learned that variational autoencoders (VAEs) can be used to visualize high-dimensional data in a meaningful, lower-dimensional space. In this kernel, I go over some details about autoencoding and autoencoders, especially VAEs, before constructing and training a deep VAE on the MNIST data from the Digit Recognizer competition. We'll see how the data cluster in the lower-dimensional space according to their digit class. Plotting the test set data in this space shows where the images with unknown digit classes fall with respect to the known digit classes.\n",
        "\n",
        "The code here borrows heavily from Fran&ccedil;ois Chollet's example VAE from his book [Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_1?ie=UTF8&qid=1520470984&sr=8-1&keywords=francois+chollet). You can find a repo of examples from the book (including the one that inspired this kernel) [here on GitHub](https://github.com/brilliantFire/deep-learning-with-python-notebooks).\n",
        "\n",
        "### *A. What is autoencoding?*\n",
        "Autoencoding is much like what it sounds in the sense that the input and 'output' are essentially the same. It's an algorithm for data compression where the functions for compression and decompression are *learned from the data*. It's considered more of a *semi-supervised* learning method as opposed to a truly *unsupervised* one since it's not entirely 'targetless'. Instead it learns the targets from the data itself.\n",
        "\n",
        "Despite all this talk of data compression, autoencoders aren't typically used for that purpose. In practice, you're much more likely to see them being used to preprocess data (as in denoising - think images but it doesn't have to be ;) ) or for dimensionality reduction. In fact, the hidden layers of simple autoencoders are doing something like principal component analysis (PCA), another method traditionally used for dimensionality reduction.\n",
        "\n",
        "### *B. Autoencoders*\n",
        "Generally autoencoders have three parts: an encoder, a decoder, and a 'loss' function that maps one to the other. For the simplest autoencoders - the sort that compress and then reconstruct the original inputs from the compressed representation - we can think of the 'loss' as describing the amount of information lost in the process of reconstruction. Typically when people are talking about autoencoders, they're talking about ones where the encoders and decoders are neural networks (in our case deep convnets). In training the autoencoder, we're optimizing the parameters of the neural networks to minimize the 'loss' (or distance) and we do that by stochastic gradient descent (yet another topic for another post). \n",
        "\n",
        "### *C. The Variational Variety*\n",
        "There's a bunch of different kinds of autoencoders but for this post I'm going to concentrate on one type called a *variational autoencoder*. Variational autoencoders (VAEs) don't learn to morph the data in and out of a compressed representation of itself like the 'vanilla' autoencoders I described above. Instead, they learn the parameters of the probability distribution that the data came from. These types of autoencoders have much in common with latent factor analysis (if you know something about that). The encoder and decoder learn models that are in terms of underlying, unobserved *latent* variables. It's essentially an inference model and a generative model daisy-chained together.\n",
        "\n",
        "![A variational autoencoder](https://i.imgur.com/ZN6MyTx.png)\n",
        "\n",
        "VAEs have received a lot of attention because of their *generative* ability (though they seem to be falling out of fashion in favor of general adversarial networks, or GANs, in that regard). Since they learn about the distribution the inputs came from, we can sample from that distribution to generate novel data. As we'll see, VAEs can also be used to cluster data in useful ways."
      ]
    },
    {
      "metadata": {
        "_uuid": "517052cecc729f77b352d0d6e74f35e8489aa474",
        "_cell_guid": "af039539-b12e-4573-a78f-b550f44c7332",
        "id": "pyXWyIomJ72H"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. *Data preparation*  \n",
        "### *A. Load Data*"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": false,
        "collapsed": true,
        "id": "UJqkfzJ8J72I"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras import metrics\n",
        "from keras import backend as K   # 'generic' backend so code works with either tensorflow or theano\n",
        "import tensorflow as tf\n",
        "K.clear_session()\n",
        "\n",
        "np.random.seed(237)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "C2Iv66alLlgL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = x_train\n",
        "X_valid = x_test\n",
        "\n",
        "# labels\n",
        "y_train = y_train\n",
        "y_valid = y_test\n"
      ],
      "metadata": {
        "id": "R0jOUOYRZVyh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "plt.imshow(x_train[i],cmap='gray')\n",
        "plt.title(y_train[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "RvIe0G-4MXd1",
        "outputId": "1c80e865-3f21-46e6-e794-1f08f4aecbab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '5')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOsUlEQVR4nO3dfayUdXrG8esqahrxBakpElbLYgxGjWUbxMaQVWNYX+JGjxqzpCY0Gtk/JHGThtTQP1bTYk19aZZqNrBRF5ot6yZqRHfjS0VlWxPiEVERF3WNZiFHqEEU8IUCd/84gz2rZ35zmHlmnvHc308yOTPPPc/MnSdcPO/zc0QIwPj3J3U3AKA3CDuQBGEHkiDsQBKEHUiCsANJEHYgCcKOUdl+3vbntvc0Hlvq7gmdIewoWRQRxzQeM+tuBp0h7EAShB0l/2z7Q9v/bfuCuptBZ8y18RiN7XMlbZa0T9IPJN0raVZE/L7WxtA2wo4xsf2kpF9HxL/V3Qvaw2Y8xiokue4m0D7Cjq+xPcn2xbb/1PYRtv9G0nclPVl3b2jfEXU3gL50pKR/knS6pAOSfifpyoh4q9au0BH22YEk2IwHkiDsQBKEHUiCsANJ9PRovG2OBgJdFhGjXg/R0Zrd9iW2t9h+x/YtnXwWgO5q+9Sb7QmS3pI0T9JWSS9Jmh8RmwvzsGYHuqwba/Y5kt6JiHcjYp+kX0q6ooPPA9BFnYR9mqQ/jHi9tTHtj9heaHvQ9mAH3wWgQ10/QBcRKyStkNiMB+rUyZp9m6STR7z+VmMagD7USdhfknSa7W/bPkrDP3Cwppq2AFSt7c34iNhve5GkpyRNkPRARLxRWWcAKtXTu97YZwe6rysX1QD45iDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgibaHbMY3w4QJE4r1448/vqvfv2jRoqa1o48+ujjvzJkzi/WbbrqpWL/rrrua1ubPn1+c9/PPPy/W77jjjmL9tttuK9br0FHYbb8nabekA5L2R8TsKpoCUL0q1uwXRsSHFXwOgC5inx1IotOwh6Snbb9se+Fob7C90Pag7cEOvwtABzrdjJ8bEdts/7mkZ2z/LiLWjXxDRKyQtEKSbEeH3wegTR2t2SNiW+PvDkmPSppTRVMAqtd22G1PtH3soeeSvidpU1WNAahWJ5vxUyQ9avvQ5/xHRDxZSVfjzCmnnFKsH3XUUcX6eeedV6zPnTu3aW3SpEnFea+++upivU5bt24t1pctW1asDwwMNK3t3r27OO+rr75arL/wwgvFej9qO+wR8a6kv6ywFwBdxKk3IAnCDiRB2IEkCDuQBGEHknBE7y5qG69X0M2aNatYX7t2bbHe7dtM+9XBgweL9euvv75Y37NnT9vfPTQ0VKx/9NFHxfqWLVva/u5uiwiPNp01O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXn2CkyePLlYX79+fbE+Y8aMKtupVKved+3aVaxfeOGFTWv79u0rzpv1+oNOcZ4dSI6wA0kQdiAJwg4kQdiBJAg7kARhB5JgyOYK7Ny5s1hfvHhxsX755ZcX66+88kqx3uonlUs2btxYrM+bN69Y37t3b7F+5plnNq3dfPPNxXlRLdbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97P3geOOO65YbzW88PLly5vWbrjhhuK81113XbG+evXqYh39p+372W0/YHuH7U0jpk22/Yzttxt/T6iyWQDVG8tm/M8lXfKVabdIejYiTpP0bOM1gD7WMuwRsU7SV68HvULSysbzlZKurLgvABVr99r4KRFxaLCsDyRNafZG2wslLWzzewBUpOMbYSIiSgfeImKFpBUSB+iAOrV76m277amS1Pi7o7qWAHRDu2FfI2lB4/kCSY9V0w6Abmm5GW97taQLJJ1oe6ukH0u6Q9KvbN8g6X1J13azyfHuk08+6Wj+jz/+uO15b7zxxmL9oYceKtZbjbGO/tEy7BExv0npoop7AdBFXC4LJEHYgSQIO5AEYQeSIOxAEtziOg5MnDixae3xxx8vznv++ecX65deemmx/vTTTxfr6D2GbAaSI+xAEoQdSIKwA0kQdiAJwg4kQdiBJDjPPs6deuqpxfqGDRuK9V27dhXrzz33XLE+ODjYtHbfffcV5+3lv83xhPPsQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE59mTGxgYKNYffPDBYv3YY49t+7uXLFlSrK9atapYHxoaKtaz4jw7kBxhB5Ig7EAShB1IgrADSRB2IAnCDiTBeXYUnXXWWcX6PffcU6xfdFH7g/0uX768WF+6dGmxvm3btra/+5us7fPsth+wvcP2phHTbrW9zfbGxuOyKpsFUL2xbMb/XNIlo0z/14iY1Xj8ptq2AFStZdgjYp2knT3oBUAXdXKAbpHt1xqb+Sc0e5PthbYHbTf/MTIAXddu2H8q6VRJsyQNSbq72RsjYkVEzI6I2W1+F4AKtBX2iNgeEQci4qCkn0maU21bAKrWVthtTx3xckDSpmbvBdAfWp5nt71a0gWSTpS0XdKPG69nSQpJ70n6YUS0vLmY8+zjz6RJk4r173//+01rre6Vt0c9XfyltWvXFuvz5s0r1serZufZjxjDjPNHmXx/xx0B6CkulwWSIOxAEoQdSIKwA0kQdiAJbnFFbb744oti/YgjyieL9u/fX6xffPHFTWvPP/98cd5vMn5KGkiOsANJEHYgCcIOJEHYgSQIO5AEYQeSaHnXG3I7++yzi/VrrrmmWD/nnHOa1lqdR29l8+bNxfq6des6+vzxhjU7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBefZxbubMmcX6okWLivWrrrqqWD/ppJMOu6exOnDgQLE+NFT+9fKDBw9W2c43Hmt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii5Xl22ydLWiVpioaHaF4RET+xPVnSQ5Kma3jY5msj4qPutZpXq3PZ8+ePNtDusFbn0adPn95OS5UYHBws1pcuXVqsr1mzpsp2xr2xrNn3S/q7iDhD0l9Lusn2GZJukfRsRJwm6dnGawB9qmXYI2IoIjY0nu+W9KakaZKukLSy8baVkq7sVpMAOndY++y2p0v6jqT1kqZExKHrFT/Q8GY+gD415mvjbR8j6WFJP4qIT+z/H04qIqLZOG62F0pa2GmjADozpjW77SM1HPRfRMQjjcnbbU9t1KdK2jHavBGxIiJmR8TsKhoG0J6WYffwKvx+SW9GxD0jSmskLWg8XyDpserbA1CVlkM2254r6beSXpd06J7BJRreb/+VpFMkva/hU287W3xWyiGbp0wpH84444wzivV77723WD/99NMPu6eqrF+/vli/8847m9Yee6y8fuAW1fY0G7K55T57RPyXpFFnlnRRJ00B6B2uoAOSIOxAEoQdSIKwA0kQdiAJwg4kwU9Jj9HkyZOb1pYvX16cd9asWcX6jBkz2uqpCi+++GKxfvfddxfrTz31VLH+2WefHXZP6A7W7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJrz7Oeee26xvnjx4mJ9zpw5TWvTpk1rq6eqfPrpp01ry5YtK857++23F+t79+5tqyf0H9bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEmvPsAwMDHdU7sXnz5mL9iSeeKNb3799frJfuOd+1a1dxXuTBmh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkhjL+OwnS1olaYqkkLQiIn5i+1ZJN0r6n8Zbl0TEb1p8Vsrx2YFeajY++1jCPlXS1IjYYPtYSS9LulLStZL2RMRdY22CsAPd1yzsLa+gi4ghSUON57ttvymp3p9mAXDYDmuf3fZ0Sd+RtL4xaZHt12w/YPuEJvMstD1oe7CjTgF0pOVm/JdvtI+R9IKkpRHxiO0pkj7U8H78P2p4U//6Fp/BZjzQZW3vs0uS7SMlPSHpqYi4Z5T6dElPRMRZLT6HsANd1izsLTfjbVvS/ZLeHBn0xoG7QwYkbeq0SQDdM5aj8XMl/VbS65IONiYvkTRf0iwNb8a/J+mHjYN5pc9izQ50WUeb8VUh7ED3tb0ZD2B8IOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTR6yGbP5T0/ojXJzam9aN+7a1f+5LorV1V9vYXzQo9vZ/9a19uD0bE7NoaKOjX3vq1L4ne2tWr3tiMB5Ig7EASdYd9Rc3fX9KvvfVrXxK9tasnvdW6zw6gd+peswPoEcIOJFFL2G1fYnuL7Xds31JHD83Yfs/267Y31j0+XWMMvR22N42YNtn2M7bfbvwddYy9mnq71fa2xrLbaPuymno72fZztjfbfsP2zY3ptS67Ql89WW4932e3PUHSW5LmSdoq6SVJ8yNic08bacL2e5JmR0TtF2DY/q6kPZJWHRpay/a/SNoZEXc0/qM8ISL+vk96u1WHOYx3l3prNsz436rGZVfl8OftqGPNPkfSOxHxbkTsk/RLSVfU0Effi4h1knZ+ZfIVklY2nq/U8D+WnmvSW1+IiKGI2NB4vlvSoWHGa112hb56oo6wT5P0hxGvt6q/xnsPSU/bftn2wrqbGcWUEcNsfSBpSp3NjKLlMN699JVhxvtm2bUz/HmnOED3dXMj4q8kXSrppsbmal+K4X2wfjp3+lNJp2p4DMAhSXfX2UxjmPGHJf0oIj4ZWatz2Y3SV0+WWx1h3ybp5BGvv9WY1hciYlvj7w5Jj2p4t6OfbD80gm7j746a+/lSRGyPiAMRcVDSz1TjsmsMM/6wpF9ExCONybUvu9H66tVyqyPsL0k6zfa3bR8l6QeS1tTQx9fYntg4cCLbEyV9T/03FPUaSQsazxdIeqzGXv5Ivwzj3WyYcdW87Gof/jwiev6QdJmGj8j/XtI/1NFDk75mSHq18Xij7t4krdbwZt3/avjYxg2S/kzSs5LelvSfkib3UW//ruGhvV/TcLCm1tTbXA1vor8maWPjcVndy67QV0+WG5fLAklwgA5IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvg/aHSyPlMbLUoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "collapsed": true,
        "id": "IPTlLcEJJ72L"
      },
      "cell_type": "code",
      "source": [
        "# train_orig = pd.read_csv('../input/train.csv')\n",
        "# test_orig = pd.read_csv('../input/test.csv')\n",
        "\n",
        "# train_orig.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee191fc918081c56e4f99a76450112e06ec298bb",
        "_cell_guid": "dfca3ccc-c861-40da-a416-08c0cb526a52",
        "id": "ki-sT7u4J72N"
      },
      "cell_type": "markdown",
      "source": [
        "### *B. Combine Train & Test*\n",
        "Let's add a placeholder 'label' column to the test dataset. We'll give all the test images a label of '11' for now (\"This data goes to 11.\") "
      ]
    },
    {
      "metadata": {
        "_uuid": "6155d58768e796e39afe51e405d60dbb14f50105",
        "_cell_guid": "d64517e9-3c29-46ca-803a-ee7d13a5bb53",
        "collapsed": true,
        "trusted": false,
        "id": "lY-4UJwlJ72O"
      },
      "cell_type": "code",
      "source": [
        "# # create 'label' column in test dataset; rearrange so that columns are in the same order as in train\n",
        "# test_orig['label'] = 11\n",
        "# testCols = test_orig.columns.tolist()\n",
        "# testCols = testCols[-1:] + testCols[:-1]\n",
        "# test_orig = test_orig[testCols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d37d4883719be67a82ebbc3d11f2a1193f38a10",
        "_cell_guid": "8d468e80-9a7f-488e-98e4-945b3a9a3744",
        "id": "FPblIY2IJ72P"
      },
      "cell_type": "markdown",
      "source": [
        "We want to train the autoencoder with as many images as possible. Also, since we don't need the labels for building the model (remember: *semi-supervised*), it makes sense to combine the train and test data into one combined dataframe."
      ]
    },
    {
      "metadata": {
        "_uuid": "4d00ef2261bf7414b8fdff34a27ceb8cc31c6a32",
        "_cell_guid": "4736824b-f96d-4276-a997-8155e3e4d13a",
        "trusted": false,
        "collapsed": true,
        "id": "7D3yHY95J72Q"
      },
      "cell_type": "code",
      "source": [
        "# # combine original train and test sets\n",
        "# combined = pd.concat([train_orig, test_orig], ignore_index = True)\n",
        "\n",
        "# combined.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f8ba5aea0756b4cf0f75add60af08abfce601194",
        "_cell_guid": "4869da15-7183-4568-b09f-0a299d0f4786",
        "trusted": false,
        "collapsed": true,
        "id": "53dOEf9EJ72R"
      },
      "cell_type": "code",
      "source": [
        "# combined.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd183a862b09ee0ec3581768ce857eb778be3c49",
        "_cell_guid": "15b23079-70ac-40fb-96a1-ad4d4dbe1d29",
        "id": "GzmrCr-6J72S"
      },
      "cell_type": "markdown",
      "source": [
        "### *C. Split into training & validation sets*\n",
        "Despite being trained in a semi-supervised way, the VAE algorithm entails minimizing a 'loss' function. As we'll see shortly, that loss is actually two different 'losses' combined, one that describes the difference between the input images and the images reconstructed from samples from the latent distribution, and another that is the difference between the latent distribution and the prior (the inputs). We can calculate this loss on a validation set with each training epoch as an estimate of how the model describes data it was not trained on.  \n",
        "\n",
        "Remember that we've combined the original train and test data so the *new* train and validation sets that we make below will each have some images with missing ('11') labels."
      ]
    },
    {
      "metadata": {
        "_uuid": "83f48e2fb61b3c738d14c2e9575d08ca8d40a9be",
        "_cell_guid": "a073bdce-1609-4542-b46c-6604db42ffe2",
        "trusted": false,
        "collapsed": true,
        "id": "VAepGbT0J72S"
      },
      "cell_type": "code",
      "source": [
        "# # Hold out 5000 random images as a validation/test sample\n",
        "# valid = combined.sample(n = 5000, random_state = 555)\n",
        "# train = combined.loc[~combined.index.isin(valid.index)]\n",
        "\n",
        "# # free up some space and delete test and combined\n",
        "# del train_orig, test_orig, combined\n",
        "\n",
        "# valid.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9caef978723aa28443dc33602e5e096f4535d750",
        "_cell_guid": "012e0e0b-8f35-4722-b6d3-22fd2619c8dd",
        "id": "TMP4oYP6J72T"
      },
      "cell_type": "markdown",
      "source": [
        "### *4. Reshape & normalize*\n",
        "Our encoder and decoder are deep convnets constructed using the Keras Functional API. We'll need to separate the inputs from the labels, normalize them by dividing the max pixel value, and reshape them into 28x28 pixel images."
      ]
    },
    {
      "metadata": {
        "_uuid": "f91d57425cd06cde12375a700a423895da9c7c3e",
        "_cell_guid": "f1e92f03-2b6c-4830-8a3e-e062f9411fdb",
        "collapsed": true,
        "trusted": false,
        "id": "9g0vso0rJ72U"
      },
      "cell_type": "code",
      "source": [
        "# # X's\n",
        "# X_train = train.drop(['label'], axis = 1)\n",
        "# X_valid = valid.drop(['label'], axis = 1)\n",
        "\n",
        "# # labels\n",
        "# y_train = train['label']\n",
        "# y_valid = valid['label']\n",
        "\n",
        "# Normalize and reshape\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "\n",
        "X_valid = X_valid.astype('float32') / 255.\n",
        "X_valid = X_valid.reshape(-1,28,28,1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b89e64ddca2b0a11ae4fe9ddfe4b928f60594c14",
        "_cell_guid": "fef54f40-7d3e-4767-bf72-d3b94509b5da",
        "id": "BbbGIyOXJ72V"
      },
      "cell_type": "markdown",
      "source": [
        "We can take a look at a few random images. The bottom right panel shows one of the more difficult-to-classify digits (even for humans!)."
      ]
    },
    {
      "metadata": {
        "_uuid": "c5f9fd1d7d3e7c4e01d73f039b90309499300b41",
        "_cell_guid": "98261ef0-11de-48f3-add1-e914a62674fd",
        "trusted": false,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "BBdTV_bzJ72W",
        "outputId": "d18f3c80-021c-4331-8bab-e684b0a8fa23"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[13][:,:,0])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[690][:,:,0])\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2375][:,:,0])\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[42013][:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5252a23c95a1ad720fe6e0681d10d91624547882",
        "_cell_guid": "2c02e436-c1c0-4d03-ae48-88253a871e10",
        "id": "aRSlbugRJ72X"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. *Model construction*\n",
        "### *A. Encoder network*\n",
        "A VAE has three basic parts:  \n",
        "\n",
        "1. An encoder that that learns the parameters (mean and variance) of the underlying latent distribution;  \n",
        "2. A means of sampling from that distribution; and,  \n",
        "3. A decoder that can turn the sample from #2 back into an image.  \n",
        "\n",
        "In this example, both the encoder and decoder networks are deep convnets. You'll notice that the encoder below has two output layers, one for the latent distribution mean (z_mu) and the other for its variance (z_log_sigma)."
      ]
    },
    {
      "metadata": {
        "_uuid": "55fea787c00da3a81b87fc655fa405dca882802a",
        "_cell_guid": "e2a1d4a9-dc41-4765-b52b-36219e13eef2",
        "collapsed": true,
        "trusted": false,
        "id": "Z4LFN-mLJ72Y"
      },
      "cell_type": "code",
      "source": [
        "img_shape = (28, 28, 1)    # for MNIST\n",
        "batch_size = 16\n",
        "latent_dim = 2  # Number of latent dimension parameters\n",
        "\n",
        "# Encoder architecture: Input -> Conv2D*4 -> Flatten -> Dense\n",
        "input_img = keras.Input(shape=img_shape)\n",
        "\n",
        "x = layers.Conv2D(32, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu')(input_img)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu',\n",
        "                  strides=(2, 2))(x)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu')(x)\n",
        "x = layers.Conv2D(64, 3,\n",
        "                  padding='same', \n",
        "                  activation='relu')(x)\n",
        "# need to know the shape of the network here for the decoder\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "\n",
        "# Two outputs, latent mean and (log)variance\n",
        "z_mu = layers.Dense(latent_dim)(x)\n",
        "z_log_sigma = layers.Dense(latent_dim)(x)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "168c43129e87eec53c6af72dba48c7a761a79063",
        "_cell_guid": "93b37058-c9d3-4bfc-b3d3-a0d1aa200965",
        "id": "WXYgPJw0J72Z"
      },
      "cell_type": "markdown",
      "source": [
        "### *B. Sampling function*\n",
        "Next, we create a function to sample from the distribution we just learned the parameters of. `epsilon` is a tensor of small random normal values. One of the assumptions underlying a VAE like this is that our data arose from a random process and is normally distributed in the latent space.\n",
        "\n",
        "With Keras, everything has to be in a 'layer' to compile correctly. This goes for our sampling function. The `Lambda` layer wrapper let's us do this."
      ]
    },
    {
      "metadata": {
        "_uuid": "b8a145b6db334a68977c89639a08ef371e54cdd6",
        "_cell_guid": "c8e1ba8f-a88a-4687-80f3-68f6815bb032",
        "collapsed": true,
        "trusted": false,
        "id": "lroVh6WlJ72Z"
      },
      "cell_type": "code",
      "source": [
        "# sampling function\n",
        "def sampling(args):\n",
        "    z_mu, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim),\n",
        "                              mean=0., stddev=1.)\n",
        "    return z_mu + K.exp(z_log_sigma) * epsilon\n",
        "\n",
        "# sample vector from the latent distribution\n",
        "z = layers.Lambda(sampling)([z_mu, z_log_sigma])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6f83916df89ac4afeee046fc2d3f817fc16c249",
        "_cell_guid": "3442d1be-3e02-44db-b615-59b010089776",
        "id": "oyIG7nEoJ72a"
      },
      "cell_type": "markdown",
      "source": [
        "### *C. Decoder network*\n",
        "The decoder is basically the encoder in reverse."
      ]
    },
    {
      "metadata": {
        "_uuid": "0df3d7471211f7403735a4e923c47a2294a0a349",
        "_cell_guid": "68322de7-9675-4788-9c60-9044cb3172db",
        "collapsed": true,
        "trusted": false,
        "id": "TL7abFglJ72b"
      },
      "cell_type": "code",
      "source": [
        "# decoder takes the latent distribution sample as input\n",
        "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
        "\n",
        "# Expand to 784 total pixels\n",
        "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activation='relu')(decoder_input)\n",
        "\n",
        "# reshape\n",
        "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "\n",
        "# use Conv2DTranspose to reverse the conv layers from the encoder\n",
        "x = layers.Conv2DTranspose(32, 3,\n",
        "                           padding='same', \n",
        "                           activation='relu',\n",
        "                           strides=(2, 2))(x)\n",
        "x = layers.Conv2D(1, 3,\n",
        "                  padding='same', \n",
        "                  activation='sigmoid')(x)\n",
        "\n",
        "# decoder model statement\n",
        "decoder = Model(decoder_input, x)\n",
        "\n",
        "# apply the decoder to the sample from the latent distribution\n",
        "z_decoded = decoder(z)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "705e156f13e4b89c73c2d5c06f0b0965d74a182f",
        "_cell_guid": "f93644c4-28cd-43dc-8e1c-f8c11b60be90",
        "id": "OqpraijpJ72b"
      },
      "cell_type": "markdown",
      "source": [
        "### *D. Loss*\n",
        "We need one more thing and that's something that will calculate the unique loss function the VAE requires. Recall that the VAE is trained using a loss function with two components:  \n",
        "\n",
        "1. 'Reconstruction loss' - This is the cross-entropy describing the errors between the decoded samples from the latent distribution and the original inputs.  \n",
        "2. The Kullback-Liebler divergence between the latent distribution and the prior (this acts as a sort of regularization term).  \n",
        "\n",
        "We define a custom layer class that calculates the loss. "
      ]
    },
    {
      "metadata": {
        "_uuid": "289b40920037fbdb305d3e76a5a08495a143693c",
        "_cell_guid": "394127bc-7beb-406a-8e2c-7aae3b436d85",
        "collapsed": true,
        "trusted": false,
        "id": "Fa5z-224J72c"
      },
      "cell_type": "code",
      "source": [
        "# construct a custom layer to calculate the loss\n",
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = K.flatten(x)\n",
        "        z_decoded = K.flatten(z_decoded)\n",
        "        # Reconstruction loss\n",
        "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "        # KL divergence\n",
        "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    # adds the custom loss to the class\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x\n",
        "\n",
        "# apply the custom loss to the input images and the decoded latent distribution sample\n",
        "y = CustomVariationalLayer()([input_img, z_decoded])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "80350c12746fdab9892122af1b60248f7567f0fe",
        "_cell_guid": "b56e4d33-5d1a-49c2-aa3e-c00692862889",
        "id": "rd3MuECwJ72c"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can instantiate the model and take a look at its summary."
      ]
    },
    {
      "metadata": {
        "_uuid": "91449af9f139e7080ddaa8b0a8152b66b9d02d95",
        "_cell_guid": "b189196e-deb3-429d-bf3b-1c8c6d4a4afb",
        "trusted": false,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzkT3DHJ72d",
        "outputId": "8a8616fd-c915-4700-cbdc-820ca2fd4f55"
      },
      "cell_type": "code",
      "source": [
        "# VAE model statement\n",
        "vae = Model(input_img, y)\n",
        "vae.compile(optimizer='rmsprop', loss=None)\n",
        "vae.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 28, 28, 32)   320         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            66          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2)            66          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 2)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 28, 28, 1)    56385       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " custom_variational_layer (Cust  (None, 28, 28, 1)   0           ['input_1[0][0]',                \n",
            " omVariationalLayer)                                              'model[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 550,629\n",
            "Trainable params: 550,629\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a838351a7a4aa74c8a1a50d09a4a4a3594aaaac1",
        "_cell_guid": "cf0514db-2761-40b7-b174-7f40d8112d91",
        "id": "l7WexedsJ72d"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. *Train the VAE*\n",
        "Finally, we fit the model."
      ]
    },
    {
      "metadata": {
        "_uuid": "e052652709cbd5fcb9e4a836306245edaaf0ab51",
        "_cell_guid": "d44843b2-3e0d-4319-bc6e-8089871ec1ea",
        "trusted": false,
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "c-cedDbhJ72d",
        "outputId": "e9c9664e-1f56-4249-a8fd-2c16f4d29a68"
      },
      "cell_type": "code",
      "source": [
        "vae.fit(x=X_train, y=None,\n",
        "        shuffle=True,\n",
        "        epochs=7,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_valid, None))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-65da520c443e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         validation_data=(X_valid, None))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        total_loss_metric_value, sample_weight=batch_dim)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/metrics.py\", line 471, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/keras_tensor.py\", line 255, in __array__\n        f'You are passing {self}, an intermediate Keras symbolic input/output, '\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum_2/Sum:0', description=\"created by layer 'tf.math.reduce_sum_2'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3e03487ab62f3f8807441ca6501438b74a5c95c6",
        "_cell_guid": "8a3a7177-31bc-462b-a43f-d79dfd79f654",
        "id": "dI5IXS78J72d"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. *Results*  \n",
        "### *A. Clustering of digits in the latent space*\n",
        "We can make predictions on the validation set using the encoder network. This has the effect of translating the images from the 784-dimensional input space into the 2-dimensional latent space. When we color-code those translated data points according to their *known* digit class, we can see how the digits cluster together."
      ]
    },
    {
      "metadata": {
        "_uuid": "ecd9333a0861a6f7aabaccf508412e9056f46035",
        "_cell_guid": "dc72b908-77e0-4ef6-859d-79f0e83d97ef",
        "collapsed": true,
        "trusted": false,
        "id": "HUkndaYeJ72e"
      },
      "cell_type": "code",
      "source": [
        "# Isolate original training set records in validation set\n",
        "valid_noTest = valid[valid['label'] != 11]\n",
        "\n",
        "# X's and Y's\n",
        "X_valid_noTest = valid_noTest.drop('label', axis=1)\n",
        "y_valid_noTest = valid_noTest['label']\n",
        "\n",
        "# Reshape and normalize\n",
        "X_valid_noTest = X_valid_noTest.astype('float32') / 255.\n",
        "X_valid_noTest = X_valid_noTest.values.reshape(-1,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d8c22694d59990b52efe2c1a6cac66b9d899412",
        "_cell_guid": "afe4a541-814e-4def-88cd-cbee97e18dfe",
        "trusted": false,
        "collapsed": true,
        "id": "gg9i6z-7J72e"
      },
      "cell_type": "code",
      "source": [
        "# Translate into the latent space\n",
        "encoder = Model(input_img, z_mu)\n",
        "x_valid_noTest_encoded = encoder.predict(X_valid_noTest, batch_size=batch_size)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(x_valid_noTest_encoded[:, 0], x_valid_noTest_encoded[:, 1], c=y_valid_noTest, cmap='brg')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c36beb9eae66ebc216ba9a8e7ee9994e776a530",
        "_cell_guid": "508868ea-ee14-4a56-9207-d4ac2607697d",
        "id": "FSNOKfkCJ72f"
      },
      "cell_type": "markdown",
      "source": [
        "Including the original test set data lets us see where they fall with respect to the known digit clusters."
      ]
    },
    {
      "metadata": {
        "_uuid": "5a1bb733976c79bf74a3a9cafa262d73e1b93865",
        "_cell_guid": "e53af49f-8591-403d-a909-5bb54947d040",
        "trusted": false,
        "collapsed": true,
        "id": "dVo8gdGnJ72f"
      },
      "cell_type": "code",
      "source": [
        "# set colormap so that 11's are gray\n",
        "custom_cmap = matplotlib.cm.get_cmap('brg')\n",
        "custom_cmap.set_over('gray')\n",
        "\n",
        "x_valid_encoded = encoder.predict(X_valid, batch_size=batch_size)\n",
        "plt.figure(figsize=(10, 10))\n",
        "gray_marker = mpatches.Circle(4,radius=0.1,color='gray', label='Test')\n",
        "plt.legend(handles=[gray_marker], loc = 'best')\n",
        "plt.scatter(x_valid_encoded[:, 0], x_valid_encoded[:, 1], c=y_valid, cmap=custom_cmap)\n",
        "plt.clim(0, 9)\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "277873274ca79a7e5abc4243609c0a169ca6c87b",
        "_cell_guid": "f592181f-07a4-4fc8-aee7-2f85922b1a1b",
        "id": "ZOp_8FWOJ72f"
      },
      "cell_type": "markdown",
      "source": [
        "### *B. Sample digits*\n",
        "Another fun thing we can do is to use the decoder network to take a peak at what samples from the latent space look like as we change the latent variables. What we end up with is a smoothly varying space where each digit transforms into the others as we dial the latent variables up and down."
      ]
    },
    {
      "metadata": {
        "_uuid": "ab1bcf744f65020eef6665b2166d32820842b824",
        "_cell_guid": "8b6e0f3e-eb67-446d-aa17-492b1c68077a",
        "trusted": false,
        "collapsed": true,
        "id": "lHnl7gxxJ72g"
      },
      "cell_type": "code",
      "source": [
        "# Display a 2D manifold of the digits\n",
        "n = 20  # figure with 20x20 digits\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "# Construct grid of latent variable values\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "# decode for each square in the grid\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
        "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='gnuplot2')\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8316c7cf88dc54ba59c793732102faa36b6aaab0",
        "_cell_guid": "36998846-4a02-43cc-b04f-4dceb90cbac0",
        "id": "rCRguz_8J72g"
      },
      "cell_type": "markdown",
      "source": [
        "Thanks for reading!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "visualizing-mnist-using-a-variational-autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}